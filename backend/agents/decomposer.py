# backend/agents/decomposer.py

from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="phi", temperature=0.3, max_tokens=120)  # ğŸ”¥ tight response

async def decompose_problem(prompt: str) -> str:
    try:
        print("ğŸŒ± Prompt sent to Ollama:\n", prompt)
        result = await llm.ainvoke(prompt)
        print("ğŸŒ¾ Result received:\n", result)
        return result
    except Exception as e:
        print("âŒ Error in decompose_problem:", e)
        return "âš ï¸ Error generating farming recommendation"
